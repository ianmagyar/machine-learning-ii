{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cvičenie 5: Value Iteration\n",
    "\n",
    "Na minulom cvičení sme aplikovali metódu policy iteration pre riešenie problému gridworld. Na tomto cvičení sa pozrieme, ako nájde riešenie metóda Value Iteration pre rovnaký problém. Aby porovnávanie bolo naozaj férové, použijeme úplne rovnaký svet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "\n",
    "Value Iteration (alebo iterácia hodnôt) môžeme vnímať ako zjednodušenú verziu metódy iterácie politiky s tým, že kým sme pri Policy Iteration mali niekoľko krokov, ktoré sa opakovali striedavo, pri Value Iteration existuje len evaluácia politiky, ktorá sa vykonáva opakovane až dovtedy, kým sa nenájde stabilný odhad. Následne sa vygeneruje finálna politika, ktorá - pri splnení predpokladov pre konvergenciu metódy - bude skoro optimálna.\n",
    "\n",
    "Úplný pseudokód algoritmu nájdete na obrázku nižšie:\n",
    "\n",
    "<img src=\"sources/lab05/value_iteration.jpg\" width=\"600\">\n",
    "<p style=\"text-align: center;\">Zdroj: Sutton-Barto: Reinforcement Learning, 2nd ed., 2018</p>\n",
    "\n",
    "Prediskutujte jednotlivé kroky a možnosti programovej aplikácie algoritmu. Následne algoritmus aplikujeme pri riešení príkladu gridworld z minulého týždňa:\n",
    "\n",
    "<img src=\"sources/lab05/gridworld_example2.jpg\" width=\"300\">\n",
    "\n",
    "Príklad predstavuje svet *3x3* s cieľovou pozíciou v pravom hornom rohu, a s jednou pascou v druhom riadku a prvom stĺpci. K dispozícii sú štyri akcie: posun na sever, východ, juh a západ. Hráč nemôže prejsť cez stenu, v tomto prípade ostane na rovnakej pozícii, ale berie sa, akokeby vykonal krok. Ak hráč sa dostane do cieľa, obdrží odmenu 10, ak spadne do pasce, tak -10. V oboch prípadoch sa hra skončí. Pre ostatné kroky dostane odmenu -1.\n",
    "\n",
    "Discount factor $\\gamma = 0.8$ a $\\theta = 1.0$. Prvý odhad hodnoty pre každý stav bude 0: $V_0 (s) = 0 \\ \\ \\  \\forall s \\in \\mathcal{S}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterácia hodnôt\n",
    "\n",
    "V prvej časti algoritmus sa snaží nájsť stabilný odhad hodnôt jednotlivých stavov, pričom platí:\n",
    "\n",
    "$V_{t+1} (s) = \\underset{a}{max} \\sum_{s'} p(s'|s, a) \\left [ r + \\gamma \\cdot V(s') \\right ]$,\n",
    "\n",
    "kde $p(s'|s, a)$ je pravdepodobnosť toho, že po vykonaní akcie $a$ zo stavu $s$ sa prostredie presunie do stavu $s'$. Keďže sa ale v našom prípade jedná o deterministický svet, táto hodnota bude vždy jedna, a pre každú kombináciu stav-akcia bude existovať iba jeden možný nasledujúci stav.\n",
    "\n",
    "Aktualizovaný odhad hodnoty stavu vieme určiť tak, že vypočítame hodnotu stavu pri vykonaní jednotlivých akcií, a následne si vyberieme tú najvyššiu hodnotu. Pre stav $s_{11}$ to bude:\n",
    "\n",
    "$V_1^N (s_{11}) = p(s_{11}, r|s_{11}, N) \\left [ r + \\gamma \\cdot V_0(s_{11}) \\right ] = 1 \\cdot \\left [ -1 + 0.8 \\cdot 0 \\right ] = -1; \\ \\ \\  \\Delta V_1^N (s_{11}) = |V_1^N (s_{11}) - V_0 (s_{11})| = |-1 - 0| = 1$\n",
    "\n",
    "$V_1^E (s_{11}) = p(s_{12}, r|s_{11}, E) \\left [ r + \\gamma \\cdot V_0(s_{12}) \\right ] = 1 \\cdot \\left [ -1 + 0.8 \\cdot 0 \\right ] = -1; \\ \\ \\  \\Delta V_1^E (s_{11}) = |V_1^E (s_{11}) - V_0 (s_{11})| = |-1 - 0| = 1$\n",
    "\n",
    "$V_1^S (s_{11}) = p(s_{21}, r|s_{11}, S) \\left [ r + \\gamma \\cdot V_0(s_{11}) \\right ] = 1 \\cdot \\left [ -10 + 0.8 \\cdot 0 \\right ] = -10; \\ \\ \\  \\Delta V_1^S (s_{11}) = |V_1^S (s_{11}) - V_0 (s_{11})| = |-10 - 0| = 10$\n",
    "\n",
    "$V_1^W (s_{11}) = p(s_{11}, r|s_{11}, W) \\left [ r + \\gamma \\cdot V_0(s_{11}) \\right ] = 1 \\cdot \\left [ -1 + 0.8 \\cdot 0 \\right ] = -1; \\ \\ \\  \\Delta V_1^W (s_{11}) = |V_1^W (s_{11}) - V_0 (s_{11})| = |-1 - 0| = 1$\n",
    "\n",
    "Potom:\n",
    "\n",
    "$V_1 (s_{11}) = -1$\n",
    "\n",
    "$\\Delta = max(0, |0 - (-1)|) = 1; \\ \\ \\ \\ \\Delta = \\theta$\n",
    "\n",
    "**Úloha:** Vypočítajte odhad hodnoty pre ostatné stavy (okrem $s_{13}$ a $s_{21}$). Použite inline prístup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generovanie výslednej politiky\n",
    "\n",
    "Na rozdiel od metódy Policy Iteration, kde sa politika vygeneruje po každej iterácii, Value Iteration nám vygeneruje politiku až na konci behu, teda až potom, čo sa ustália odhady hodnôt stavov. Spôsob výberu je skoro rovnaký ako výpočet hodnoty stavu, avšak výsledok nie je hodnota stavu pre dvojicu stav-akcia, ale samotná akcia:\n",
    "\n",
    "$\\pi (s) = \\underset{a}{argmax} \\sum_{s'} p(s'|s, a) \\left [ r + \\gamma \\cdot V(s') \\right ]$.\n",
    "\n",
    "Ak odhad hodnôt stavov konverguje, potom vyprodukovaná politika bude (skoro) optimálna. V prípade nášho problému politiku by mali tvoriť akcie, ktoré hráča posunú viac smerom k cieľovej pozícii z jednotlivých stavov, napr.:\n",
    "\n",
    "$\\pi (s_{11}) = \\{E\\}$; $\\pi (s_{12}) = \\{E\\}$; $\\pi (s_{22}) = \\{N\\}$; $\\pi (s_{23}) = \\{N\\}$; $\\pi (s_{31}) = \\{E\\}$; $\\pi (s_{32}) = \\{N\\}$; $\\pi (s_{33}) = \\{N\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementácia\n",
    "\n",
    "Overte správnosť vašich výpočtov pomocou [ukážkovej implementácie metódy Value Iteration](sources/lab05/value_iteration.py) a spozorujte, ako sa metóda dopracuje k optimálnej politiky. Porovnajte výsledky s riešením [metódou Policy Iteration](sources/lab05/policy_iteration_det.py) a s výpočtom [Bellmanových rovníc](sources/lab05/bellman_gridworld.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domáca úloha\n",
    "\n",
    "Doteraz pri výpočte nových hodnôt stavov sme použili in-place stratégiu. Upravte kód tak, aby implementoval sweep stratégiu, a porovnajte výsledky po jednotlivých iteráciach metódy Value Iteration pri použití in-place, resp. sweep stratégie výpočtov."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
