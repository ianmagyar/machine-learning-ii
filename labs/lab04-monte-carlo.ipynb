{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cvičenie 4: Monte Carlo metódy\n",
    "\n",
    "Na predošlom cvičení sme videli, ako fungujú metódy Policy Iteration a Value Iteration, ktoré ale stále predpokladali úplnú znalosť dynamiky prostredia. Na dnešnom cvičení sa pozrieme na Monte Carlo metódy, konkrétne na on-policy first-visit Monte Carlo algoritmus odhadu $\\pi^*$. Pracovať pritom budeme s rovnakým svetom gridworld."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metóda Monte Carlo\n",
    "\n",
    "Na rozdiel od predošlých dvoch metód založených na dynamickom programovaní, Monte Carlo metódy sa učia z priamych interakcií s prostredím, práve preto ich priebeh závisí od konkrétnych epizódach interakcie. Vďaka tomu tieto metódy vieme aplikovať aj vtedy, ak nepoznáme úplny model, teda prechody medzi stavmi nám nie sú známe, vieme ich ale nasimulovať. Algoritmus zároveň pracuje explicitne s $\\varepsilon$-soft politikou, ktorej príkladom je aj $\\varepsilon$-greedy politika, teda akcia s najväčšou očakávanou hodnotou sa vyberie najčastejšie, ale s istou pravdepodobnosťou bude výber akcie náhodný.\n",
    "\n",
    "Pseudokód algoritmu nájdete na obrázku nižšie, prediskutujte jeho kroky a význam premenných a parametrov, s ktorými algoritmus pracuje.\n",
    "\n",
    "<img src=\"sources/lab04/monte_carlo.jpg\" width=\"600\">\n",
    "<p style=\"text-align: center;\">Zdroj: Sutton-Barto: Reinforcement Learning, 2nd ed., 2018</p>\n",
    "\n",
    "Následne algoritmus aplikujeme pri riešení už známeho príkladu gridworld:\n",
    "\n",
    "<img src=\"sources/lab04/gridworld_mdp.jpg\" width=\"600\">\n",
    "\n",
    "Príklad predstavuje svet *3x3* s cieľovou pozíciou v pravom hornom rohu, a s jednou pascou v strede svete. K dispozícii sú štyri akcie: posun na sever, východ, juh a západ. Ak sa hráč dostane do cieľa, obdrží odmenu 10, ak spadne do pasce, tak -10. V oboch prípadoch sa hra skončí. Pre ostatné kroky dostane odmenu -1.\n",
    "\n",
    "Dolná a pravá časť sveta je úplne deterministická, pričom na pozíciach označených svetlomodrou farbou fúka silný vietor, ktorý môže agenta posunúť na východ aj keď sa pohybuje iným smerom. Pravdepodobnosť pohybu vo vybranom smere je v týchto prípadoch $0.6$, pravdepodobnosť posunutia na východ je $0.4$. Ak agent vyberie pohyb na východ, určite sa tam dostane. Ak sa pohybuje na západ, ostane na svojej pôvodnej pozícii (vietor a jeho pohyb sa rušia).\n",
    "\n",
    "Discount factor $\\gamma = 0.8$. Prvý odhad hodnoty pre každú dvojicu stav-akcia bude 0: $Q(s, a) = 0 \\ \\ \\  \\forall s \\in \\mathcal{S}, a \\in \\mathcal{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ukážka fungovania algoritmu\n",
    "\n",
    "Majme epizódu:\n",
    "\n",
    "$s_{33}, E, -1, s_{33}, S, -1, s_{33}, E, -1, s_{33}, E, -1, s_{33}, E, -1, s_{33}, N, -1, s_{23}, E, -1, s_{23}, E, -1, s_{23}, N, 10$,\n",
    "\n",
    "ktorá predstavuje úspešnú interakciu, keďže agent sa dostal na cieľovú pozíciu.\n",
    "\n",
    "Pri náhodnej politike ($\\pi(a|s) = 0.25 \\ \\  \\forall s \\in \\mathcal{S}, a \\in \\mathcal{A}$) a s hodnotami $\\varepsilon = 0.1$  a $G=0$ sa táto epizóda spracuje nasledovne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Krok $s_{23}, N, 10$\n",
    "\n",
    "Vyhodnotenie začneme od posledného kroku. Najprv sa vypočíta nová hodnota $G$:\n",
    "\n",
    "$G \\leftarrow \\gamma \\cdot G + R = 0.8 \\cdot 0 + 10 = 10$\n",
    "\n",
    "Ďalej zistíme, či sa kombinácia stavu a akcie $s_{23}, N$ nachádza v predošlých krokoch epizódy. Keďže takýto krok nebol vykonaný skôr v epizóde, urobíme ďalšie zmeny v hodnotách premenných. Najprv si zapamätáme skúsenosť:\n",
    "\n",
    "$Returns(s_{23}, N) = \\{ 10 \\}$\n",
    "\n",
    "Potom aktualizujeme odhad hodnoty dvojice stav-akcia:\n",
    "\n",
    "$Q(s_{23}, N) = \\frac{\\sum Returns(s_{23}, N)}{|Returns(s_{23}, N)|} = 10$\n",
    "\n",
    "$Q(s_{23}) = \\{ 10; 0; 0; 0 \\}$\n",
    "\n",
    "Akcia s najväčšou očakávanou hodnotou je teda $N$, na základe čoho aktualizujeme politiku pre daný stav:\n",
    "\n",
    "$\\pi(N|s_{23}) = 1 - \\varepsilon + \\frac{\\varepsilon}{|\\mathcal{A}|} = 1 - 0.1 + \\frac{0.1}{4} = 0.925$\n",
    "\n",
    "$\\pi(E|s_{23}) = \\frac{\\varepsilon}{|\\mathcal{A}|} = \\frac{0.1}{4} = 0.025$\n",
    "\n",
    "$\\pi(S|s_{23}) = \\frac{\\varepsilon}{|\\mathcal{A}|} = \\frac{0.1}{4} = 0.025$\n",
    "\n",
    "$\\pi(W|s_{23}) = \\frac{\\varepsilon}{|\\mathcal{A}|} = \\frac{0.1}{4} = 0.025$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Krok $s_{23}, E, -1$\n",
    "\n",
    "Pokračujeme predchádzajúcim krokom epizódy. Najprv sa aktualizuje hodnota $G$:\n",
    "\n",
    "$G \\leftarrow \\gamma \\cdot G + R = 0.8 \\cdot 10 - 1 = 7$\n",
    "\n",
    "Znova zisťujeme, či sa kombinácia stavu a akcie $s_{23}, E$ nachádza v predošlých krokoch epizódy. Keďže takýto krok bol vykonaný skôr v epizóde, ďalšie zmeny zatiaľ nerobíme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Krok $s_{23}, E, -1$\n",
    "\n",
    "Pokračujeme predchádzajúcim krokom epizódy. Najprv sa aktualizuje hodnota $G$:\n",
    "\n",
    "$G \\leftarrow \\gamma \\cdot G + R = 0.8 \\cdot 7 - 1 = 4.6$\n",
    "\n",
    "Znova zisťujeme, či sa kombinácia stavu a akcie $s_{23}, E$ nachádza v predošlých krokoch epizódy. Keďže takýto krok nebol vykonaný skôr v epizóde, urobíme ďalšie zmeny v hodnotách premenných. Najprv si zapamätáme skúsenosť:\n",
    "\n",
    "$Returns(s_{23}, E) = \\{ 4.6 \\}$\n",
    "\n",
    "Potom aktualizujeme odhad hodnoty dvojice stav-akcia:\n",
    "\n",
    "$Q(s_{23}, E) = \\frac{\\sum Returns(s_{23}, E)}{|Returns(s_{23}, E)|} = 4.6$\n",
    "\n",
    "$Q(s_{23}) = \\{ 10; 4.6; 0; 0 \\}$\n",
    "\n",
    "Akcia s najväčšou očakávanou hodnotou ostáva $N$, na základe čoho aktualizujeme politiku pre daný stav:\n",
    "\n",
    "$\\pi(N|s_{23}) = 1 - \\varepsilon + \\frac{\\varepsilon}{|\\mathcal{A}|} = 1 - 0.1 + \\frac{0.1}{4} = 0.925$\n",
    "\n",
    "$\\pi(E|s_{23}) = \\frac{\\varepsilon}{|\\mathcal{A}|} = \\frac{0.1}{4} = 0.025$\n",
    "\n",
    "$\\pi(S|s_{23}) = \\frac{\\varepsilon}{|\\mathcal{A}|} = \\frac{0.1}{4} = 0.025$\n",
    "\n",
    "$\\pi(W|s_{23}) = \\frac{\\varepsilon}{|\\mathcal{A}|} = \\frac{0.1}{4} = 0.025$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Krok $s_{33}, N, -1$\n",
    "\n",
    "Pokračujeme predchádzajúcim krokom epizódy. Najprv sa aktualizuje hodnota $G$:\n",
    "\n",
    "$G \\leftarrow \\gamma \\cdot G + R = 0.8 \\cdot 4.6 - 1 = 2.68$\n",
    "\n",
    "Keďže krok $s_{33}, N$ nebol vykonaný skôr v epizóde, urobíme ďalšie zmeny v hodnotách premenných. Najprv si zapamätáme skúsenosť:\n",
    "\n",
    "$Returns(s_{33}, N) = \\{ 2.68 \\}$\n",
    "\n",
    "Potom aktualizujeme odhad hodnoty dvojice stav-akcia:\n",
    "\n",
    "$Q(s_{33}) = \\{ 2.68; 0; 0; 0 \\}$\n",
    "\n",
    "Akcia s najväčšou očakávanou hodnotou bude $N$, na základe čoho aktualizujeme politiku pre daný stav:\n",
    "\n",
    "$\\pi(N|s_{33}) = 1 - \\varepsilon + \\frac{\\varepsilon}{|\\mathcal{A}|} = 1 - 0.1 + \\frac{0.1}{4} = 0.925$\n",
    "\n",
    "$\\pi(E|s_{33}) = \\frac{\\varepsilon}{|\\mathcal{A}|} = \\frac{0.1}{4} = 0.025$\n",
    "\n",
    "$\\pi(S|s_{33}) = \\frac{\\varepsilon}{|\\mathcal{A}|} = \\frac{0.1}{4} = 0.025$\n",
    "\n",
    "$\\pi(W|s_{33}) = \\frac{\\varepsilon}{|\\mathcal{A}|} = \\frac{0.1}{4} = 0.025$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Krok $s_{33}, E, -1$\n",
    "\n",
    "Pokračujeme predchádzajúcim krokom epizódy. Najprv sa aktualizuje hodnota $G$:\n",
    "\n",
    "$G \\leftarrow \\gamma \\cdot G + R = 0.8 \\cdot 2.68 - 1 = 1.144$\n",
    "\n",
    "Takýto krok bol vykonaný skôr niekoľkokrát v tejto epizóde, takže túto aktualizáciu urobíme až pri prvom výskyte tejto kombinácie v epizóde."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Krok $s_{33}, E, -1$\n",
    "\n",
    "Pokračujeme predchádzajúcim krokom epizódy. Najprv sa aktualizuje hodnota $G$:\n",
    "\n",
    "$G \\leftarrow \\gamma \\cdot G + R = 0.8 \\cdot 1.144 - 1 = -0.0848$\n",
    "\n",
    "Takýto krok bol vykonaný skôr niekoľkokrát v tejto epizóde, takže túto aktualizáciu urobíme až pri prvom výskyte tejto kombinácie v epizóde."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Krok $s_{33}, E, -1$\n",
    "\n",
    "Pokračujeme predchádzajúcim krokom epizódy. Najprv sa aktualizuje hodnota $G$:\n",
    "\n",
    "$G \\leftarrow \\gamma \\cdot G + R = 0.8 \\cdot (-0.0848) - 1 = -1.06784$\n",
    "\n",
    "Takýto krok bol vykonaný skôr v tejto epizóde, takže túto aktualizáciu urobíme až pri prvom výskyte tejto kombinácie v epizóde."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Krok $s_{33}, S, -1$\n",
    "\n",
    "Nasleduje spracovanie druhého kroku epizódy. Najprv sa aktualizuje hodnota $G$:\n",
    "\n",
    "$G \\leftarrow \\gamma \\cdot G + R = 0.8 \\cdot (-1.06784) - 1 = -1.854272$\n",
    "\n",
    "Skúsenosť si zapamätáme, a aktualizujeme ostatné hodnoty:\n",
    "\n",
    "$Returns(s_{33}, S) = \\{ -1.854272 \\}$\n",
    "\n",
    "$Q(s_{33}) = \\{ 2.68; 0; -1.85; 0 \\}$\n",
    "\n",
    "Akcia s najväčšou očakávanou hodnotou ostáva N."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Krok $s_{33}, E, -1$\n",
    "\n",
    "Ostáva nám prvý krok epizódy, ktorý je samozrejme prvým výskytom kombinácie $(s_{33}, E)$. Najprv sa aktualizuje hodnota $G$:\n",
    "\n",
    "$G \\leftarrow \\gamma \\cdot G + R = 0.8 \\cdot (-1.854272) - 1 = -2.4834176$\n",
    "\n",
    "Skúsenosť si zapamätáme, a aktualizujeme ostatné hodnoty:\n",
    "\n",
    "$Returns(s_{33}, E) = \\{ -2.48 \\}$\n",
    "\n",
    "$Q(s_{33}) = \\{ 2.68; -2.48; -1.85; 0 \\}$\n",
    "\n",
    "Akcia s najväčšou očakávanou hodnotou ostáva N:\n",
    "\n",
    "$\\pi(N|s_{33}) = 1 - \\varepsilon + \\frac{\\varepsilon}{|\\mathcal{A}|} = 1 - 0.1 + \\frac{0.1}{4} = 0.925$\n",
    "\n",
    "$\\pi(E|s_{33}) = \\frac{\\varepsilon}{|\\mathcal{A}|} = \\frac{0.1}{4} = 0.025$\n",
    "\n",
    "$\\pi(S|s_{33}) = \\frac{\\varepsilon}{|\\mathcal{A}|} = \\frac{0.1}{4} = 0.025$\n",
    "\n",
    "$\\pi(W|s_{33}) = \\frac{\\varepsilon}{|\\mathcal{A}|} = \\frac{0.1}{4} = 0.025$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Úloha:\n",
    "\n",
    "Vypočítajte aktualizované hodnoty pomocou metódy Monte Carlo na nasledujúcich epizódach (svet je rovnaký):\n",
    "\n",
    "$s_{33}, E, -1, s_{33}, E, -1, s_{33}, W, -1, s_{32}, W, -1, s_{31}, W, -1, s_{31}, W, -1, s_{31}, E, -1, s_{32}, S, -1, s_{32}, N, -10$\n",
    "\n",
    "$s_{11}, S, -1, s_{12}, E, 10$\n",
    "\n",
    "$s_{12}, N, -1, s_{12}, W, -1, s_{11}, W, -1, s_{11}, N, -1, s_{12}, E, 10$\n",
    "\n",
    "Vaše riešenie si skontrolujte pomocou [ukážkovej implementácie metódy](sources/lab04/monte_carlo.py). Existuje možnosť, že metóda nájde optimálne riešenie už po prvej epizóde?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Môžete si vyskúšať aj [alternatívnu implementáciu metódy](sources/lab04/MCAgent.zip), ktorá prirodzenejšie modeluje interakciu medzi agentom a prostredím."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
