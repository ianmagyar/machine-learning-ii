{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cvičenie 8: Q-siete\n",
    "\n",
    "Na minulom cvičení sme videli, ako fungujú temporal difference algoritmy Q-Learning a SARSA. Síce Q-Learning garantovane konverguje k riešeniu, k tomu vyžaduje, aby jednotlivé Q-hodnoty sa aktualizovali nekonečne veľakrát. Postačujúcu aproximáciu dosiahneme už aj s menším počtom aktualizácií, avšak ako rastie veľkosť stavového priestoru, tak rastú aj nároky na počet interakcií, po ktorých Q-Learning nájde politiku s požadovanou mierou vhodnosti. Riešením tohto problému (a podobne veľkého priestoru akcií) je implementácia Q-Learningu pomocou neurónových sietí."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Q-siete\n",
    "\n",
    "V štandardnej implementácii algoritmu Q-Learning sa Q-hodnoty ukladajú v tabuľke. Je ale zrejmé, že pre veľký stavový priestor alebo veľký počet akcií je použitie tabuľky nereálne. Q-hodnoty sa ale aktualizujú podľa istého pravidla, a odhady môžeme vnímať ako výsledok tzv. Q-funkcie, a túto funkciu vieme aproximovať nielen tabuľkou, ale aj pomocou umelej neurónovej siete. Počet vstupných neurónov je pritom daný reprezentáciou stavu (alebo pozorovania), a počet výstupných neurónov je daný počtom možných akcií.\n",
    "\n",
    "[Na tomto odkaze nájdete jednoduchú implementáciu Q-siete](sources/lab08/q-network.py), ktorá je priama transformácia štandardného Q-Learningu pre použitie v neurónových sieťach. Na obrázku nižšie nájdete pseudokód tejto implementácie.\n",
    "\n",
    "<img src=\"sources/lab08/q-network.jpg\" width=\"750\">\n",
    "\n",
    "Prediskutujte jednotlivé kroky a spôsob implementácie pomocou neurónovej siete. Následne spustite trénovanie na ukážkovom prostredí [CartPole](https://gym.openai.com/envs/CartPole-v1/). K spustení potrebujete mať nainštalované knižnice `TensorFlow` a `Keras`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Q-siete s pamäťou\n",
    "\n",
    "Veľkým nedostatkom naivnej implementácie je to, že kým neurónové siete sa natrénujú ideálne ak neexistuje sekvenčný súvis medzi trénovacími príkladmi, tak epizódy v rámci učenia posilňovaním sú prirodzene sekvenčné. Pre vyriešenie tohto problému sa môže použiť pamäť skúseností, z ktorého sa náhodným (alebo stochastickým) spôsobom vyberie niekoľko záznamov pri každom trénovacom kroku. Takéto riešenie odstráni negatívny efekt sekvenčných skúseností, a popritom máme lepšiu efektivitu učenia, keďže tá istá skúsenosť sa použije viackrát na trénovanie neurónovej siete.\n",
    "\n",
    "Upravený algoritmus trénovania nájdete nižšie.\n",
    "\n",
    "<img src=\"sources/lab08/q-network-replay.jpg\" width=\"750\">\n",
    "\n",
    "**Úloha:** Rozšírte implementáciu Q-siete o pamäť skúseností a upravte spôsob trénovania. Môžete pritom zadefinovať nové funkcie alebo rozšíriť už existujúce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Použitie cieľových sietí\n",
    "\n",
    "Druhou stabilizačnou metódou Q-sietí je použitie cieľových (*target*) sietí, ktoré majú rovnakú topológiu ako primárna sieť, avšak aktualizujú sa menej často, zvyčajne iba na konci epizódy. Cieľom trénovania je potom nájsť také riešenie, v ktorom tieto dve siete konvergujú.\n",
    "\n",
    "Algoritmus trénovania s použitím cieľových sietí vidíte nižšie.\n",
    "\n",
    "<img src=\"sources/lab08/q-network-target.jpg\" width=\"750\">\n",
    "\n",
    "**Úloha:** Doplnte implementáciu Q-sietí aby používala cieľovú sieť pri trénovaní. Následne vytvorte funkciu, ktorá bude aktualizovať váhy tejto siete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Podpora spojitého priestoru akcií\n",
    "\n",
    "V predošlých krokoch ste videli Q-siete, ktoré podporujú prácu so spojitými stavovými priestormi. Pre podporu spojitého priestoru akcií potrebujeme použiť trocha iný prístup, a to algoritmy typu *actor-critic*. V týchto algoritmoch používame dva typy agentov: jeden je actor, teda agent, ktorý vyberá akcie; druhý agent je critic, ktorého cieľom je vyhodnotiť správnosť a vhodnosť akcií vybratých actorom. V závislosti od implementácie títo agenti môžu byť implementovaní pomocou cieľových sietí, čo reálne znamená prácu so štyrmi sieťami.\n",
    "\n",
    "Príkladom algoritmu actor-critic s podporou spojitého priestoru akcií je `DDPG`, teda *deep deterministic policy gradient*. Vstupom pre sieť actora je reprezentácia stavu, a výstupom je hodnota, ktorá určuje akciu. Napríklad, ak máme úlohu riadenia vozidla, a jedna z akcií nastavuje rýchlosť vozidla v intervale 0 - 130, tak actor bude generovať číslo z tohto intervalu. Critic ako vstup berie stavovú reprezentáciu, a akciu agenta, teda výstup actora. Výstupom critic siete je jedno číslo, a to očakávaná Q-hodnota danej akcie pri danom stave prostredia.\n",
    "\n",
    "Algoritmus trénovania nájdete nižšie.\n",
    "\n",
    "<img src=\"sources/lab08/ddpg.jpg\" width=\"750\">\n",
    "\n",
    "Fungovanie `DDPG` agenta si ukážeme na prostredí [Pendulum](https://gym.openai.com/envs/Pendulum-v0/). [Stiahnite si ukážkovú implementáciu](sources/lab08/ddpg.py), a prediskutujte kód a jednotlivé kroky potrebné k dosiahnutiu riešenia."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
