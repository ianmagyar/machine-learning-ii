{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cvičenie 4: Value Iteration\n",
    "\n",
    "Už ste videli, ako sa aplikuje metóda Policy Iteration pre riešenie problému gridworld. Ďalej sa pozrieme na to, ako nájde riešenie metóda Value Iteration pre rovnaký problém. Aby porovnávanie bolo naozaj férové, použijeme úplne rovnaký svet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "\n",
    "Value Iteration (alebo *iterácia hodnôt*) môžeme vnímať ako zjednodušenú verziu metódy iterácie politiky s tým, že kým sme pri Policy Iteration mali niekoľko krokov, ktoré sa opakovali striedavo, pri Value Iteration existuje len evaluácia politiky, ktorá sa vykonáva opakovane až dovtedy, kým sa nenájde stabilný odhad. Následne sa vygeneruje finálna politika, ktorá - pri splnení predpokladov pre konvergenciu metódy - bude skoro optimálna.\n",
    "\n",
    "Úplný pseudokód algoritmu nájdete na obrázku nižšie:\n",
    "\n",
    "<img src=\"lab04/value_iteration.jpg\" width=\"600\">\n",
    "<p style=\"text-align: center;\">Zdroj: Sutton-Barto: Reinforcement Learning, 2nd ed., 2018</p>\n",
    "\n",
    "Prediskutujte jednotlivé kroky a možnosti programovej implementácie algoritmu. Následne algoritmus aplikujeme pri riešení príkladu gridworld z minulého týždňa:\n",
    "\n",
    "<img src=\"lab04/gridworld_mdp.jpg\" width=\"600\">\n",
    "\n",
    "Príklad predstavuje svet *3x3* s cieľovou pozíciou v pravom hornom rohu, a s jednou pascou v strede sveta. K dispozícii sú štyri akcie: posun na sever, východ, juh a západ. Ak sa hráč dostane do cieľa, obdrží odmenu 10, ak spadne do pasce, tak -10. V oboch prípadoch sa hra ukončí. Pre ostatné kroky dostane agent odmenu -1. Pre stav označený $s_{32}$ môže byť odmena 5 s pravdepodobnosťou *0.1*.\n",
    "\n",
    "Väčšina sveta je úplne deterministická. Na pozícii $s_{12}$ fúka silný vietor, ktorý môže agenta posunúť na juh aj keď sa pohybuje iným smerom. Pravdepodobnosť pohybu vo vybranom smere je v týchto prípadoch $0.6$, pravdepodobnosť posunutia na juh je $0.4$. Ak agent vyberie pohyb na juh, určite sa tam dostane.\n",
    "\n",
    "Discount factor $\\gamma = 0.8$ a $\\theta = 0.1$. Začneme s politikou $\\pi(s) = N \\ \\ \\forall \\ \\ s \\in \\mathcal{S}$, teda na každej pozícii sa vyberie pohyb na sever."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterácia hodnôt\n",
    "\n",
    "V prvej časti sa algoritmus snaží nájsť stabilný odhad hodnôt jednotlivých stavov, pričom platí:\n",
    "\n",
    "$V(s) = \\underset{a}{max} \\sum_{s', r} p(s', r|s, a) \\left [ r + \\gamma \\cdot V(s') \\right ]$,\n",
    "\n",
    "kde $p(s', r|s, a)$ je pravdepodobnosť toho, že po vykonaní akcie $a$ zo stavu $s$ sa prostredie presunie do stavu $s'$ a agent obdrží odmenu $r$.\n",
    "\n",
    "Aktualizovaný odhad hodnoty stavu vieme určiť tak, že vypočítame hodnotu stavu pri vykonaní jednotlivých akcií, a následne si vyberieme tú najvyššiu hodnotu. Pre stav $s_{11}$ to bude:\n",
    "\n",
    "$V^N (s_{11}) = p(s_{11}, r_{s}|s_{11}, N) \\left [ r_{s} + \\gamma \\cdot V(s_{11}) \\right ] = 1 \\cdot \\left [ -1 + 0.8 \\cdot 0 \\right ] = -1$\n",
    "\n",
    "$V^E (s_{11}) = p(s_{12}, r_{s}|s_{11}, E) \\left [ r_{s} + \\gamma \\cdot V(s_{12}) \\right ] = 1 \\cdot \\left [ -1 + 0.8 \\cdot 0 \\right ] = -1$\n",
    "\n",
    "$V^S (s_{11}) = p(s_{21}, r_{s}|s_{11}, S) \\left [ r_{s} + \\gamma \\cdot V(s_{21}) \\right ] = 1 \\cdot \\left [ -1 + 0.8 \\cdot 0 \\right ] = -1$\n",
    "\n",
    "$V^W (s_{11}) = p(s_{11}, r_{s}|s_{11}, W) \\left [ r_{s} + \\gamma \\cdot V(s_{11}) \\right ] = 1 \\cdot \\left [ -1 + 0.8 \\cdot 0 \\right ] = -1$\n",
    "\n",
    "Potom:\n",
    "\n",
    "$V(s_{11}) = -1$\n",
    "\n",
    "$\\Delta = max(0, |0 - (-1)|) = 1; \\ \\ \\ \\ \\Delta > \\theta$\n",
    "\n",
    "Podobne pre $s_{12}$:\n",
    "\n",
    "$V^N (s_{12}) = p(s_{12}, r_{s}|s_{12}, N) \\left [ r_{s} + \\gamma \\cdot V(s_{12}) \\right ] + p(s_{22}, r_{t}|s_{12}, N) \\left [ r_{t} + \\gamma \\cdot V(s_{22}) \\right ] = 0.6 \\cdot \\left [ -1 + 0.8 \\cdot 0 \\right ] + 0.4 \\cdot \\left [ -10 + 0.8 \\cdot 0 \\right ] = -4.6$\n",
    "\n",
    "$V^E (s_{12}) = p(s_{13}, r_{g}|s_{11}, E) \\left [ r_{g} + \\gamma \\cdot V(s_{13}) \\right ] + p(s_{22}, r_{t}|s_{12}, E) \\left [ r_{t} + \\gamma \\cdot V(s_{22}) \\right ] = 0.6 \\cdot \\left [ 10 + 0.8 \\cdot 0 \\right ] + 0.4 \\cdot \\left [ -10 + 0.8 \\cdot 0 \\right ] = 2$\n",
    "\n",
    "$V^S (s_{12}) = p(s_{22}, r_{t}|s_{11}, S) \\left [ r_{t} + \\gamma \\cdot V(s_{22}) \\right ] = 1 \\cdot \\left [ -10 + 0.8 \\cdot 0 \\right ] = -10$\n",
    "\n",
    "$V^W (s_{12}) = p(s_{11}, r_{s}|s_{11}, W) \\left [ r_{s} + \\gamma \\cdot V(s_{11}) \\right ] + p(s_{22}, r_{t}|s_{12}, W) \\left [ r_{t} + \\gamma \\cdot V(s_{22}) \\right ] = 0.6 \\cdot \\left [ -1 + 0.8 \\cdot (-1) \\right ] + 0.4 \\cdot \\left [ -10 + 0.8 \\cdot 0 \\right ] = -5.08$\n",
    "\n",
    "Potom:\n",
    "\n",
    "$V(s_{12}) = 2$\n",
    "\n",
    "$\\Delta = max(1, |0 - 2|) = 2; \\ \\ \\ \\ \\Delta > \\theta$\n",
    "\n",
    "**Úloha:** Vypočítajte hodnoty pre zvyšné stavy (okrem $s_{13}$ a $s_{22}$) na základe nižšie uvedených hodnôt. Použite inline prístup.\n",
    "\n",
    "$V(s_{11}) = 0.6$\n",
    "\n",
    "$V(s_{12}) = 2$\n",
    "\n",
    "$V(s_{13}) = 0.0$\n",
    "\n",
    "$V(s_{21}) = -0.52$\n",
    "\n",
    "$V(s_{22}) = 0.0$\n",
    "\n",
    "$V(s_{23}) = 10.0$\n",
    "\n",
    "$V(s_{31}) = 3.28$\n",
    "\n",
    "$V(s_{32}) = 4.6$\n",
    "\n",
    "$V(s_{33}) = 7$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generovanie výslednej politiky\n",
    "\n",
    "Na rozdiel od metódy Policy Iteration, kde sa politika vygeneruje po každej iterácii, Value Iteration nám vygeneruje politiku až na konci behu, teda až potom, čo sa ustália odhadované hodnoty stavov. Spôsob výberu je skoro rovnaký ako výpočet hodnoty stavu, avšak výsledok nie je hodnota stavu pre dvojicu stav-akcia, ale samotná akcia:\n",
    "\n",
    "$\\pi (s) = \\underset{a}{argmax} \\sum_{s', r} p(s', r|s, a) \\left [ r + \\gamma \\cdot V(s') \\right ]$.\n",
    "\n",
    "Ak odhad hodnôt stavov konverguje, potom vyprodukovaná politika bude (skoro) optimálna. V prípade nášho problému politiku by mali tvoriť akcie, ktoré hráča posunú viac smerom k cieľovej pozícii z jednotlivých stavov, napr.:\n",
    "\n",
    "$\\pi (s_{11}) = \\{E\\}$; $\\pi (s_{12}) = \\{E\\}$; $\\pi (s_{21}) = \\{S\\}$; $\\pi (s_{23}) = \\{N\\}$; $\\pi (s_{31}) = \\{E\\}$; $\\pi (s_{32}) = \\{E\\}$; $\\pi (s_{33}) = \\{N\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementácia\n",
    "\n",
    "Overte správnosť vašich výpočtov pomocou [ukážkovej implementácie metódy Value Iteration](lab04/value_iteration.py) a spozorujte, ako sa metóda dopracuje k optimálnej politiky.\n",
    "\n",
    "**Domáca úloha:** Upravte kód tak, aby implementoval sweep stratégiu."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
