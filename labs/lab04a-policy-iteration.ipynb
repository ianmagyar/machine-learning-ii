{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cvičenie 4: Policy Iteration\n",
    "\n",
    "Na minulom cvičení ste videli, ako sa dajú vyriešiť Markovovské rozhodovacie procesy pomocou Bellmanových rovníc. Od tohto týždňa sa pozrieme na rôzne metódy učenia posilňovaním, pričom ich budeme aplikovať na rovnaký gridworld problém pre porovnávanie. Prvá metóda bude práve Policy Iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "\n",
    "Policy Iteration pozostáva z iteratívnej aplikácie dvoch krokov, a to vyhodnocovanie (*policy evaluation*) a zlepšovanie (*policy improvement*). Pseudokód algoritmu nájdete na obrázku nižšie.\n",
    "\n",
    "<img src=\"lab04/policy_iteration.jpg\" width=\"600\">\n",
    "<p style=\"text-align: center;\">Zdroj: Sutton-Barto: Reinforcement Learning, 2nd ed., 2018</p>\n",
    "\n",
    "Prediskutujte význam jednotlivých operácií a krokov a následne aplikujte algoritmus na ukážkový príklad gridworldu. Ako ukážku, nižšie nájdete vypracované kroky pre jeden stav.\n",
    "\n",
    "<img src=\"lab04/gridworld_mdp.jpg\" width=\"600\">\n",
    "\n",
    "Príklad predstavuje svet *3x3* s cieľovou pozíciou v pravom hornom rohu, a s jednou pascou v strede sveta. K dispozícii sú štyri akcie: posun na sever, východ, juh a západ. Ak sa hráč dostane do cieľa, obdrží odmenu 10, ak spadne do pasce, tak -10. V oboch prípadoch sa hra ukončí. Pre ostatné kroky dostane agent odmenu -1. Pre stav označený $s_{32}$ môže byť odmena 5 s pravdepodobnosťou *0.1*.\n",
    "\n",
    "Väčšina sveta je úplne deterministická. Na pozícii $s_{12}$ fúka silný vietor, ktorý môže agenta posunúť na juh aj keď sa pohybuje iným smerom. Pravdepodobnosť pohybu vo vybranom smere je v týchto prípadoch $0.6$, pravdepodobnosť posunutia na juh je $0.4$. Ak agent vyberie pohyb na juh, určite sa tam dostane.\n",
    "\n",
    "Discount factor $\\gamma = 0.8$ a $\\theta = 0.1$. Začneme s politikou $\\pi(s) = N \\ \\ \\forall \\ \\ s \\in \\mathcal{S}$, teda na každej pozícii sa vyberie pohyb na sever."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Policy Evaluation\n",
    "\n",
    "Pre stav $s_{11}$ (prvý riadok, prvý stĺpec) vieme vypočítať hodnotu stavu tak, že berieme do úvahy všetky možné prechody pri výbere akcie $N$. Na základe pravdepodobností prechodov vieme vypočítať hodnotu tohto stavu:\n",
    "\n",
    "$V(s_{11}) = p \\left ( s_{11}, r_{s} | s_{11}, \\pi(s_{11}) \\right ) \\cdot (r_{s} + \\gamma \\cdot V(s_{11})) $\n",
    "\n",
    "$V(s_{11}) = 1.0 \\cdot (-1 + 0.8 \\cdot 0) $\n",
    "\n",
    "$V(s_{11}) = -1$\n",
    "\n",
    "Zmena odhadu pre $V(s_{11})$ je $\\Delta V(s_{11}) = |0 - (-1)| = 1$, čo je väčšie ako $\\theta$, takže už teraz je zrejmé, že vyhodnocovanie si budeme musieť zopakovať.\n",
    "\n",
    "V našom svete pre nedeterministickú pozíciu $s_{12}$ existujú dva možné prechody: $s_{12}$ a $s_{22}$. Na základe pravdepodobností prechodov vieme vypočítať hodnotu tohto stavu:\n",
    "\n",
    "$V(s_{12}) = p \\left ( s_{12}, r_{s} | s_{12}, \\pi(s_{12}) \\right ) \\cdot (r_{s} + \\gamma \\cdot V(s_{12})) + p \\left ( s_{22}, r_{t} | s_{12}, \\pi(s_{12}) \\right ) \\cdot (r_{t} + \\gamma \\cdot V(s_{22})) $\n",
    "\n",
    "$V(s_{11}) = 0.6 \\cdot (-1 + 0.8 \\cdot 0) + 0.4 \\cdot (-10 + 0.8 \\cdot 0) $\n",
    "\n",
    "$V(s_{11}) = -4.6$\n",
    "\n",
    "Zmena odhadu pre $V(s_{12})$ je $\\Delta V(s_{12}) = |0 - (-4.6)| = 4.6$, takže hodnotu najväčšej zmeny aktualizujeme.\n",
    "\n",
    "**Úloha:** Vypočítajte hodnotu ďalších stavov (okrem $s_{13}$ a $s_{22}$). Použite pritom inplace prístup.\n",
    "\n",
    "**Úloha:** Vypočítajte hodnotu ďalších stavov na základe nasledujúcich hodnôt, ktoré by ste dostali po niekoľkých iteráciách:\n",
    "\n",
    "$V(s_{11}) = -2.952$\n",
    "\n",
    "$V(s_{12}) = -8.377$\n",
    "\n",
    "$V(s_{13}) = 0.0$\n",
    "\n",
    "$V(s_{21}) = -3.362$\n",
    "\n",
    "$V(s_{22}) = 0.0$\n",
    "\n",
    "$V(s_{23}) = 10.0$\n",
    "\n",
    "$V(s_{31}) = -3.689$\n",
    "\n",
    "$V(s_{32}) = -10.0$\n",
    "\n",
    "$V(s_{33}) = 7.0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Policy Improvement\n",
    "\n",
    "Začiatočnú politiku $\\pi_0$ definujeme ľubovoľne, v našom prípade to bola politika posunu na sever v každom stave. Ak už máme vypočítane hodnoty stavov, tieto hodnoty vieme použiť pri zlepšovaní politiky. Pre každú akciu vypočítame očakávanú dlhodobú odmenu podľa vzorca:\n",
    "\n",
    "$q(s, a) = \\sum_{s', r} p(s', r|s, a) \\cdot (r + \\gamma \\cdot V(s'))$\n",
    "\n",
    "Vyhodnocovanie politiky končilo nasledovne:\n",
    "\n",
    "$V(s_{11}) = -3.362$\n",
    "\n",
    "$V(s_{12}) = -8.621$\n",
    "\n",
    "$V(s_{13}) = 0.0$\n",
    "\n",
    "$V(s_{21}) = -3.689$\n",
    "\n",
    "$V(s_{22}) = 0.0$\n",
    "\n",
    "$V(s_{23}) = 10.0$\n",
    "\n",
    "$V(s_{31}) = -3.951$\n",
    "\n",
    "$V(s_{32}) = -10.0$\n",
    "\n",
    "$V(s_{33}) = 7.0$\n",
    "\n",
    "Teda pre stav $s_{12}$:\n",
    "\n",
    "$q(s_{12}, N) = p(s_{12}, r_{s} | s_{12}, N) \\cdot (r_{s} + \\gamma \\cdot V(s_{12})) + p(s_{22}, r_{t} | s_{12}, N) \\cdot  (r_{t} + \\gamma \\cdot V(s_{22})) = 0.6 \\cdot (-1 + 0.8 \\cdot -8.621) + 0.4 \\cdot (-10 + 0.8 \\cdot 0) = -8.738 $\n",
    "\n",
    "$q(s_{12}, E) = p(s_{13}, r_{g} | s_{12}, E) \\cdot (r_{g} + \\gamma \\cdot V(s_{13})) + p(s_{22}, r_{t} | s_{12}, E) \\cdot  (r_{t} + \\gamma \\cdot V(s_{22})) = 0.6 \\cdot (10 + 0.8 \\cdot 0) + 0.4 \\cdot (-10 + 0.8 \\cdot 0) = 2.0 $\n",
    "\n",
    "$q(s_{12}, S) = p(s_{22}, r_{t} | s_{12}, S) \\cdot  (r_{t} + \\gamma \\cdot V(s_{22})) = 1.0 \\cdot (-10 + 0.8 \\cdot 0) = -10 $\n",
    "\n",
    "$q(s_{12}, W) = p(s_{11}, r_{s} | s_{12}, W) \\cdot (r_{s} + \\gamma \\cdot V(s_{11})) + p(s_{22}, r_{t} | s_{12}, W) \\cdot  (r_{t} + \\gamma \\cdot V(s_{22})) = 0.6 \\cdot (-1 + 0.8 \\cdot (-3.362)) + 0.4 \\cdot (-10 + 0.8 \\cdot 0) = -6.214 $\n",
    "\n",
    "pre jednotlivé akcie. Následne aplikujeme funkciu $argmax$, ktorá nám vráti akciu, pri ktorej je hodnota maximálna:\n",
    "\n",
    "$\\pi(s_{12}) = argmax_a(q(s_{12}, N), q(s_{12}, E), q(s_{12}, S), q(s_{12}, W)) = E$\n",
    "\n",
    "**Úloha:** Vypočítajte všetky očakávané odmeny pre dvojice stav-akcia (okrem stavov $s_{13}$ a $s_{22}$) a na základe hodnôt aktualizujte politiku $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementácia\n",
    "\n",
    "Vaše výpočty overte na [ukážkovej implementácii metódy Policy Iteration](lab04/policy_iteration.py) a spozorujte, ako sa metóda dopracuje k optimálnej politike.\n",
    "\n",
    "**Domáca úloha:** Implementáciu opravte tak, aby namiesto inplace stratégie používala sweep stratégiu."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
