{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cvičenie 7: Q-Learning a SARSA\n",
    "\n",
    "Na dnešnom cvičení sa budeme zaoberať algoritmami typu temporal difference, a to konkrétne Q-učením a algoritmom SARSA. Algoritmy budeme aplikovať na riešenie už známeho prostredia Gridworld."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-učenie\n",
    "\n",
    "Q-učenie (*Q-Learning*), podobne ako Monte Carlo metódy, sa učí priamo z interakcií s prostredím, jeho priebeh teda závisí od priebehu konkrétnych epizód. Q-Learning zároveň nevyžaduje známy model prostredia, prechody medzi stavmi sa buď nasimulujú, alebo určia zo skutočnej interakcie agenta s prostredím. Algoritmus zvyčajne pracuje s $\\varepsilon$-greedy politikou, kde s najväčšou pravdepodobnosťou sa vyberie akcia, ktorú agent vníma ako najlepšiu možnú, pritom ale existuje možnosť na výber náhodnej akcie. Q-learning je ale off-policy metóda, to znamená, že pri aktualizácii odhadu hodnôt dvojice stav-akcia sa neberie do úvahy práve platná politika agenta.\n",
    "\n",
    "Pseudokód nájdete na obrázku nižšie, prediskutujte jeho kroky a význam premenných a parametrov, s ktorými algoritmus pracuje.\n",
    "\n",
    "<img src=\"sources/lab07/q-learning.jpg\">\n",
    "\n",
    "Následne algoritmus aplikujeme pri riešení už známeho príkladu gridworld:\n",
    "\n",
    "<img src=\"sources/lab07/gridworld_example2.jpg\" width=\"300\">\n",
    "\n",
    "Príklad predstavuje svet *3x3* s cieľovou pozíciou v pravom hornom rohu, a s jednou pascou v druhom riadku a prvom stĺpci. K dispozícii sú štyri akcie: posun na sever, východ, juh a západ. Hráč nemôže prejsť cez stenu, v tomto prípade ostane na rovnakej pozícii, ale berie sa, akokeby vykonal krok. Ak sa hráč dostane do cieľa, obdrží odmenu 10, ak spadne do pasce, tak -10. V oboch prípadoch sa hra skončí. Pre ostatné kroky dostane agent odmenu -1.\n",
    "\n",
    "Discount factor $\\gamma = 0.8$. Učiaci parameter $\\alpha = 0.2$; $\\varepsilon = 0.1$. Prvý odhad hodnoty pre každú dvojicu stav-akcia bude 0: $Q(s, a) = 0 \\ \\ \\  \\forall s \\in \\mathcal{S}, a \\in \\mathcal{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ukážka fungovania algoritmu\n",
    "\n",
    "Priebeh algoritmu si ukážeme na nasledovných epizódach:\n",
    "\n",
    "$s_{22}, W, -10, s_{21}$\n",
    "\n",
    "$s_{31}, S, -1, s_{31}, E, -1, s_{32}, W, -1, s_{31}, N, -10, s_{21}$\n",
    "\n",
    "$s_{12}, S, -1 s_{22}, S, -1, s_{32}, N, -1, s_{22}, N, -1, s_{12}, E, 10, s_{13}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. epizóda\n",
    "\n",
    "Prvá epizóda sa skladá iba z jedného kroku, pri ktorom aktualizujeme iba hodnotu $Q(s_{22}, W)$. K tomu okrem získaných údajov potrebujeme len vedieť maximálnu Q-hodnotu pre nasledujúci stav $s_{21}$. Keďže sa jedná o terminálny stav, platí:\n",
    "\n",
    "$\\underset{a}{max} Q(s_{21}, a) = 0.0$\n",
    "\n",
    "A teda pri aktualizácii platí:\n",
    "\n",
    "$Q(s_{22}, W) \\leftarrow Q(s_{22}, W) + \\alpha \\cdot (r + \\gamma \\cdot \\underset{a}{max}Q(s_{21}, a) - Q(s_{22}, W))$\n",
    "\n",
    "$Q(s_{22}, W) \\leftarrow 0.0 + 0.2 \\cdot (-10 + 0.8 \\cdot 0.0 - 0.0)$\n",
    "\n",
    "$Q(s_{22}, W) \\leftarrow -2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. epizóda\n",
    "\n",
    "Pri Q-learningu vykonáme aktualizáciu hodnôt pre každý krok, teda pre druhú epizódu platia rovnaké výpočty:\n",
    "\n",
    "$Q(s_{31}, S) \\leftarrow Q(s_{31}, S) + \\alpha \\cdot (r + \\gamma \\cdot \\underset{a}{max}Q(s_{31}, a) - Q(s_{31}, S))$\n",
    "\n",
    "$Q(s_{31}, S) \\leftarrow 0.0 + 0.2 \\cdot (-1 + 0.8 \\cdot 0.0 - 0.0)$\n",
    "\n",
    "$Q(s_{31}, S) \\leftarrow -0.2$\n",
    "\n",
    "Pre druhý krok:\n",
    "\n",
    "$Q(s_{31}, E) \\leftarrow Q(s_{31}, E) + \\alpha \\cdot (r + \\gamma \\cdot \\underset{a}{max}Q(s_{32}, a) - Q(s_{31}, E))$\n",
    "\n",
    "$Q(s_{31}, E) \\leftarrow 0.0 + 0.2 \\cdot (-1 + 0.8 \\cdot 0.0 - 0.0)$\n",
    "\n",
    "$Q(s_{31}, E) \\leftarrow -0.2$\n",
    "\n",
    "Pre tretí krok:\n",
    "\n",
    "$Q(s_{32}, W) \\leftarrow Q(s_{32}, W) + \\alpha \\cdot (r + \\gamma \\cdot \\underset{a}{max}Q(s_{31}, a) - Q(s_{32}, W))$\n",
    "\n",
    "$Q(s_{32}, W) \\leftarrow 0.0 + 0.2 \\cdot (-1 + 0.8 \\cdot 0.0 - 0.0)$\n",
    "\n",
    "$Q(s_{32}, W) \\leftarrow -0.2$\n",
    "\n",
    "A pre posledný krok:\n",
    "\n",
    "$Q(s_{31}, N) \\leftarrow Q(s_{31}, N) + \\alpha \\cdot (r + \\gamma \\cdot \\underset{a}{max}Q(s_{21}, a) - Q(s_{31}, N))$\n",
    "\n",
    "$Q(s_{31}, N) \\leftarrow 0.0 + 0.2 \\cdot (-10 + 0.8 \\cdot 0.0 - 0.0)$\n",
    "\n",
    "$Q(s_{31}, N) \\leftarrow 2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. epizóda\n",
    "\n",
    "Spracujte 3. ukážkovú epizódu a skontrolujte získané Q-hodnoty podľa tabuľky:\n",
    "\n",
    "| stav |   N  |   E  |   S  |   W  |\n",
    "|:----:|:----:|:----:|:----:|:----:|\n",
    "| s11  |    0 |    0 |    0 |    0 |\n",
    "| s12  |    0 |    2 | -0.2 |    0 |\n",
    "| s13  |    0 |    0 |    0 |    0 |\n",
    "| s21  |    0 |    0 |    0 |    0 |\n",
    "| s22  | -0.2 |    0 | -0.2 |   -2 |\n",
    "| s23  |    0 |    0 |    0 |    0 |\n",
    "| s31  |   -2 | -0.2 | -0.2 |    0 |\n",
    "| s32  | -0.2 |    0 |    0 | -0.2 |\n",
    "| s33  |    0 |    0 |    0 |    0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ďalší priebeh algoritmu si môžete vizualizovať pomocou [ukážkového kódu](sources/lab07/q-learning.zip). Porovnajte vypočítané Q-hodnoty s riešením Bellmanových rovníc optimality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA\n",
    "\n",
    "On-policy variantom algoritmu Q-Learning je SARSA, ktorý oproti Q-Learningu berie do úvahy aj nasledujúcu akciu pri aktualizácii Q-hodnoty. Prediskutujte jednotlivé kroky algoritmu a porovnajte ich s krokmi algoritmu Q-Learning.\n",
    "\n",
    "<img src=\"sources/lab07/sarsa.jpg\">\n",
    "\n",
    "Následne si stiahnite [implementáciu algoritmu SARSA](sources/lab07/sarsa.py), ktorá ale obsahuje rovnaký kód, ako Q-learning. Opravte kód tak, aby implementoval algoritmus SARSA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domáca úloha\n",
    "\n",
    "Pri aplikácii algoritmov Q-Learning a SARSA sa často používa takzvaný $\\varepsilon$-decay, kde na začiatku trénovania algoritmus má vysokú hodnotu $\\varepsilon$ (často $1$), ktorá sa časom zmenšuje tak, že sa vynásobí konštantou $decay < 1$. Podobným štýlom sa dá prispôsobovať aj učiaci parameter počas trénovania. Rozšírte implementáciu Q-Learningu (alebo SARSA) o takýto decay, a porovnajte mieru konvergencie na danej mape. Výsledky porovnania napíšte do komentárov. Riešenie odovzdávate cez MS Teams."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
